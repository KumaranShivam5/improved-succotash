{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_assign2_q3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpe7Zcmu2saCsLKWfru56l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KumaranShivam5/improved-succotash/blob/master/assignment2/ML_assign2_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqBkDX_7apuA",
        "colab_type": "text"
      },
      "source": [
        "**SPAM/HAM classification using Naive Bays classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3PxIztagYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPWQ4LfQjvKs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeEuoyxbboZ",
        "colab_type": "code",
        "outputId": "9b1b4ea7-128b-4e1c-a219-975bd87b1feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "## reading the data from file ##\n",
        "data=pd.read_csv('spam.csv', sep=\",\" , encoding='latin-1')\n",
        "#print(data)\n",
        "data=data.drop([ 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1)\n",
        "data=data.rename(columns={'v1':'ctg','v2':'content'})\n",
        "data.describe()\n",
        "data.groupby('ctg').describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">content</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ctg</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>4825</td>\n",
              "      <td>4516</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>747</td>\n",
              "      <td>653</td>\n",
              "      <td>Please call our customer service representativ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     content                                                               \n",
              "       count unique                                                top freq\n",
              "ctg                                                                        \n",
              "ham     4825   4516                             Sorry, I'll call later   30\n",
              "spam     747    653  Please call our customer service representativ...    4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRjSHZFcrhCs",
        "colab_type": "text"
      },
      "source": [
        "Train  test Split routine\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Iyih8rg_LY",
        "colab_type": "code",
        "outputId": "3a29d4c4-f101-49a3-ad2d-ba8481ae1d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#tokenizing words\n",
        "#converting to lowercase\n",
        "#removing stopwords\n",
        "\n",
        "import string as st\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def process(message):\n",
        "    m_void=''\n",
        "    for char in message:\n",
        "        if char not in st.punctuation:\n",
        "            m_void+=(char)\n",
        "    m_void_token=[word.lower() for word in m_void.split()]\n",
        "    m_void_token=[word for word in m_void_token if word not in stopwords.words('english')]\n",
        "    return(m_void_token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpwKEZyerkwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#randomixing data before splitting\n",
        "data=data.sample(frac=1).reset_index(drop=\"true\")\n",
        "#print(data)\n",
        "split=0.8\n",
        "split_no=int(len(data)*split)\n",
        "train_msg=data.iloc[0:split_no]['content']\n",
        "train_labels=data.iloc[0:split_no]['ctg']\n",
        "test_msg=data.iloc[split_no+1:]['content']\n",
        "test_labels=data.iloc[split_no+1:]['ctg']\n",
        "#print(train_data.iloc[100].content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzyg7c0hnFtW",
        "colab_type": "text"
      },
      "source": [
        "**Text-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E9tHxMhwXlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##VECTORIZE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect=CountVectorizer(analyzer=process)\n",
        "\n",
        "#using only the training data for generating\n",
        "#bag of words\n",
        "train_data=count_vect.fit_transform(train_msg)\n",
        "test_data=count_vect.transform(test_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uQ5ZOU3e_bJ",
        "colab_type": "text"
      },
      "source": [
        "**Designing my own NB classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liSH1KmCe-sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nb_fit(train_data,train_labels):\n",
        "    ##normalixing frequencies\n",
        "    train_data_arr=train_data.toarray()\n",
        "    train_data_arr_norm=np.where(train_data_arr > 0, 1, 0)\n",
        "    #vector of occurence of each words in spam\n",
        "    spam_train_vector_norm=train_data_arr_norm[train_labels=='spam'].sum(axis=0)\n",
        "    #vector of occurence of each words in ham\n",
        "    ham_train_vector_norm=train_data_arr_norm[train_labels=='ham'].sum(axis=0)\n",
        "\n",
        "\n",
        "    #calculating prior probabilities\n",
        "    no_of_spam=len(train_labels[train_labels=='spam'])\n",
        "    no_of_ham=len(train_labels[train_labels=='ham'])\n",
        "    no_of_features=len(train_data_arr_norm[0])\n",
        "    p_spam=np.log(np.divide(no_of_spam,(no_of_spam+no_of_ham)))\n",
        "    p_ham=np.log(np.divide(no_of_ham,(no_of_spam+no_of_ham)))\n",
        "    \n",
        "    #calculating log likelihood of each words\n",
        "    log_lik_spam=np.log(np.divide(spam_train_vector_norm +1, no_of_spam+no_of_features))\n",
        "    log_lik_ham=np.log(np.divide(ham_train_vector_norm +1 , no_of_ham+no_of_features))\n",
        "    #returns -- log-prior(ham), log-prior(spam)\n",
        "    #-----log-likelihood(each words| ham),\n",
        "    #-----log-likelihood(each_eords|spam)\n",
        "    return(p_ham,p_spam,log_lik_ham,log_lik_spam)\n",
        "\n",
        "def predict(test_data,model):\n",
        "    test_data_arr=test_data.toarray()\n",
        "    test_result=[]\n",
        "    for i in range(len(test_data_arr)):\n",
        "        #dot product is interesting here\n",
        "        #by this we are actually adding the\n",
        "        #log-likelihood of only the words \n",
        "        #that are present in the test message\n",
        "        #as test message is also encoded in the\n",
        "        # dictionary format as per the train data\n",
        "        p_ham=model[0]+np.dot(test_data_arr[i],model[2])\n",
        "        p_spam=model[1]+np.dot(test_data_arr[i],model[3])\n",
        "        if(p_ham>p_spam):\n",
        "            test_result.append('ham')\n",
        "        else:\n",
        "            test_result.append('spam')\n",
        "    return(test_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j8yKT_2xtYH",
        "colab_type": "code",
        "outputId": "ba768049-6c81-45dd-cab3-5ee4e6ab741e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model=nb_fit(train_data,train_labels)\n",
        "\n",
        "test_prediction=predict(test_data,model)\n",
        "print(np.asarray(test_prediction).shape)\n",
        "print(test_labels.values.shape)\n",
        "acc_mat=test_labels.values==np.asarray(test_prediction)\n",
        "accuracy=len(acc_mat[acc_mat==True])/len(acc_mat)\n",
        "print(accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1114,)\n",
            "(1114,)\n",
            "97.48653500897666\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}